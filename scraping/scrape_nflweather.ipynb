{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T20:17:38.044461Z",
     "start_time": "2017-12-13T20:17:37.719436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T20:17:38.741076Z",
     "start_time": "2017-12-13T20:17:38.617266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_page(url, year, week):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        pagecontent = resp.content\n",
    "        soup  = html.fromstring(pagecontent)\n",
    "        table = soup.xpath('//table')[0]\n",
    "        rows  = table.xpath('./tbody/tr')\n",
    "        teams = [row.xpath('./td[@class=\"team-img\"]/a/@href') for row in rows]\n",
    "        team1 = [re.sub('/en/team/','',team[0]) for team in teams]\n",
    "        team2 = [re.sub('/en/team/','',team[1]) for team in teams]\n",
    "        wind_speeds = clean_fields([row.xpath('./td[12]/text()') for row in rows])\n",
    "        forecasts = clean_fields([row.xpath('./td[10]/text()') for row in rows])\n",
    "        header_row = ['team1','team2','wind_conditions','weather_forecast']\n",
    "        rows_data = list(zip(team1,team2,wind_speeds,forecasts))\n",
    "\n",
    "        with open(\"../data/NflWeather/\"+str(year)+\"_\"+str(week)+\".csv\", \"w\") as f:\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow(header_row)\n",
    "            wr.writerows(rows_data)\n",
    "        \n",
    "    else:\n",
    "        print(\"STATUS:\",url)\n",
    "\n",
    "def parse_td(row):\n",
    "    teams = row.xpath('./td[@class=\"team-img\"]/a/@href')\n",
    "    wind_speed = row.xpath('./td[12]/text()')\n",
    "    new_rows = []\n",
    "    for i, matchup in enumerate(teams):\n",
    "        new_row = [matchup]+[wind_speed[i]]\n",
    "    return new_rows\n",
    "\n",
    "def clean_fields(rows):\n",
    "    new_rows = []\n",
    "    for row in rows:\n",
    "        for field in row:\n",
    "            new_rows.append(field.strip())\n",
    "    return new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T20:18:14.348626Z",
     "start_time": "2017-12-13T20:18:14.345294Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YEARS = [2013,2014,2015,2016]\n",
    "CURR_WEEK = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-13T20:19:34.505036Z",
     "start_time": "2017-12-13T20:18:14.601680Z"
    }
   },
   "outputs": [],
   "source": [
    "root_url = 'http://nflweather.com/en/week/{}/week-{}/'\n",
    "for year in YEARS:\n",
    "    for week in range(1,CURR_WEEK):\n",
    "        url = root_url.format(year, week)\n",
    "        scrape_page(url, year, week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis]",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
